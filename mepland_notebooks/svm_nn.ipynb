{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run code.py\n",
    "%matplotlib inline\n",
    "time_all_start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_file_name = 'data/all_el.root'\n",
    "bkg_file_name = 'data/all_mu.root'\n",
    "\n",
    "# data\n",
    "# sig_tree  = 'electron_tags'\n",
    "# bkg_tree  = 'muons'\n",
    "\n",
    "# MC\n",
    "sig_tree  = 'electron_mc'\n",
    "bkg_tree  = 'muon_mc'\n",
    "\n",
    "fit_verbose = 1\n",
    "\n",
    "output_path = 'plots'\n",
    "make_path(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create eProbabilityHT curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_eprob = uproot.open(sig_file_name)[sig_tree].array('eProbHT')\n",
    "mu_eprob = uproot.open(bkg_file_name)[bkg_tree].array('eProbHT')\n",
    "\n",
    "m_eprob = min(el_eprob.shape[0], mu_eprob.shape[0])\n",
    "el_eprob = el_eprob[:m_eprob]\n",
    "mu_eprob = mu_eprob[:m_eprob]\n",
    "print('Using %.2g el, %.2g mu for eProbHT' % (el_eprob.shape[0], mu_eprob.shape[0]))\n",
    "\n",
    "roc_eprob_obj = eprob_roc_generateor(el_eprob, mu_eprob)\n",
    "\n",
    "roc_eprob = [roc_eprob_obj.tpr(), roc_eprob_obj.fpr(), 'eProbHT', 'eprob', 'black', '-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup variables to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = OrderedDict(p='default',\n",
    "                        pT='default',\n",
    "                        eta='symmetric',\n",
    "                        nTRThitsMan='default',\n",
    "                        nTRTouts='default',\n",
    "                        fHTMB='default',\n",
    "                        fAr='default',\n",
    "                        trkOcc='default',\n",
    "                        sumToTsumL='default')\n",
    "\n",
    "# TODO formerly just [k for k in variables], but was getting out of order for mepland in python2.7\n",
    "hard_coded_order = ['p',\n",
    "    'pT',\n",
    "    'eta',\n",
    "    'nTRThitsMan',\n",
    "    'nTRTouts',\n",
    "    'fHTMB',\n",
    "    'fAr',\n",
    "    'trkOcc',\n",
    "    'sumToTsumL']\n",
    "\n",
    "# other vars\n",
    "# lep_pT, phi\n",
    "# nTRThits, nArhits, nXehits, nHThitsMan, nPrechitsMan, NhitsdEdx, sumToT, sumL, PHF\n",
    "\n",
    "# all the hit_ vars\n",
    "# eProbHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_df, bkg_df, X_train, X_test, y_train, y_test = create_df_tts_scale(\n",
    "    sig_file_name, sig_tree,\n",
    "    bkg_file_name, bkg_tree,\n",
    "    hard_coded_order,\n",
    "    test_size=0.2,\n",
    "    # test_size=0.333333,\n",
    "    # sig_n=50000,\n",
    "    # bkg_n=50000,\n",
    "    shuffle=True,\n",
    "    scale_style={i:v for i,(_,v) in enumerate(variables.items())}\n",
    ")\n",
    "\n",
    "# val_data=None\n",
    "val_data=(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(X_train.shape)\n",
    "    print(X_train[0])\n",
    "    print(y_train.shape)\n",
    "    print(y_train[0])\n",
    "    print(X_train.shape[1])\n",
    "\n",
    "print(\"Training on m = %.2g\" % (y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(3,3,figsize=(10,10))\n",
    "fortitles = {0:['$p$',axarr[0,0]],\n",
    "             1:['$p_\\mathrm{T}$',axarr[0,1]],\n",
    "             2:['$\\eta$',axarr[0,2]],\n",
    "             3:['nTRT',axarr[1,0]],\n",
    "             4:['nTRT outs',axarr[1,1]],\n",
    "             5:['Fraction HTMB',axarr[1,2]],\n",
    "             6:['Fraction Ar',axarr[2,0]],\n",
    "             7:['Track Occ.',axarr[2,1]],\n",
    "             8:['$\\sum\\mathrm{ToT}/\\sum L$',axarr[2,2]]}\n",
    "\n",
    "for k,v in fortitles.items():\n",
    "    v[1].hist([X_train[:,k][y_train>0.5],X_train[:,k][y_train<0.5]],label=['Sig','Bkg'],bins=30,histtype='step',normed=True)\n",
    "    v[1].set_xlabel(v[0])\n",
    "    \n",
    "fig.legend(['Muons','Electrons'])\n",
    "plt.tight_layout()\n",
    "fig.savefig(output_path+'/allvars.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scale_example(sig_file_name,sig_tree,output_path,'p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_m = min(50000, y_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "svm1 = svm.SVC(#C=1.0, #kernel='rbf', #tol=0.001, #gamma='auto',\n",
    "    probability=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "time_svm1_start = datetime.now()\n",
    "\n",
    "svm1.fit(X_train[:svm_m],y_train[:svm_m]);\n",
    "\n",
    "time_svm1_stop = datetime.now()\n",
    "print(\"svm1 training elapsed time: %s\" % (strfdelta(time_svm1_stop-time_svm1_start, \"{hours} hours, {minutes} minutes, {seconds} seconds\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier_1D_output(svm1.decision_function(X_test[y_test>0.5]), # el\n",
    "                          svm1.decision_function(X_test[y_test<0.5]), # mu\n",
    "                          'SVM', 'svm', output_path\n",
    "                          #, 'Default sklearn.svm.SVC settings'\n",
    "                         )\n",
    "\n",
    "fpr_svm1, tpr_svm1, thresholds_svm1 = roc_curve(y_test, svm1.decision_function(X_test))\n",
    "roc_svm1 = [tpr_svm1, fpr_svm1, 'SVM', 'svm', 'blue', ':']\n",
    "\n",
    "plot_roc([roc_eprob, roc_svm1], output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras / Tensorflow work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import load_model\n",
    "\n",
    "input_ndimensions = X_train.shape[1] # n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_model_default = 'models/model_default'\n",
    "train_load_model_default = train_or_load(fname_model_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_load_model_default == 'n':\n",
    "    \n",
    "    # create\n",
    "    model_default = Sequential()\n",
    "    model_default.add(Dense(12, input_dim=input_ndimensions, activation='relu'))\n",
    "    model_default.add(Dense(8, activation='relu'))\n",
    "    model_default.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model_default.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # train\n",
    "    train_start = datetime.now()\n",
    "    hist_model_default = model_default.fit(X_train, y_train,\n",
    "                                           epochs=5, batch_size=50,\n",
    "                                           verbose=fit_verbose, validation_data=val_data);\n",
    "\n",
    "    hist_dict_model_default = hist_model_default.history\n",
    "    print(strfdelta(datetime.now()-train_start, \"Training time: {hours} hours, {minutes} minutes, {seconds} seconds\"))\n",
    "\n",
    "    # save model to HDF5, history to pickle\n",
    "    model_default.save(fname_model_default+'.h5')\n",
    "   \n",
    "    with open(fname_model_default+'_hist.pickle', 'wb') as handle:\n",
    "        pickle.dump(hist_dict_model_default, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "else:\n",
    "    # load model from HDF5, history from pickle\n",
    "    model_default = load_model(fname_model_default+'.h5')\n",
    "    \n",
    "    with open(fname_model_default+'_hist.pickle', 'rb') as handle:\n",
    "        hist_dict_model_default = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss_vs_epoch(hist_dict_model_default, 'NN (Default)', 'nn_default', output_path, True, False)\n",
    "plot_acc_loss_vs_epoch(hist_dict_model_default, 'NN (Default)', 'nn_default', output_path, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model_default %s: %.2f%%\" % (model_default.metrics_names[1], model_default.evaluate(X_test,y_test,verbose=0)[1]*100))\n",
    "\n",
    "plot_classifier_1D_output(model_default.predict(X_test[y_test>0.5], verbose=0), # el\n",
    "                          model_default.predict(X_test[y_test<0.5], verbose=0), # mu\n",
    "                          'NN (Default)', 'nn_default', output_path\n",
    "                         )\n",
    "\n",
    "fpr_model_default, tpr_model_default, thresholds_model_default = roc_curve(y_test, model_default.predict(X_test, verbose=0))\n",
    "roc_model_default = [tpr_model_default, fpr_model_default, 'NN (Default)', 'nn_default', 'magenta', '--']\n",
    "\n",
    "plot_roc([roc_eprob, roc_model_default], output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_model_wide = 'models/model_wide'\n",
    "train_load_model_wide = train_or_load(fname_model_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_load_model_wide == 'n':\n",
    "    \n",
    "    # create\n",
    "    model_wide = Sequential()\n",
    "    model_wide.add(Dense(24, input_dim=input_ndimensions, activation='relu'))\n",
    "    model_wide.add(Dense(16, activation='relu'))\n",
    "    model_wide.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model_wide.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # train\n",
    "    train_start = datetime.now()\n",
    "    hist_model_wide = model_wide.fit(X_train, y_train,\n",
    "                                           epochs=5, batch_size=50,\n",
    "                                           verbose=fit_verbose, validation_data=val_data);\n",
    "\n",
    "    hist_dict_model_wide = hist_model_wide.history\n",
    "    print(strfdelta(datetime.now()-train_start, \"Training time: {hours} hours, {minutes} minutes, {seconds} seconds\"))\n",
    "\n",
    "    # save model to HDF5, history to pickle\n",
    "    model_wide.save(fname_model_wide+'.h5')\n",
    "   \n",
    "    with open(fname_model_wide+'_hist.pickle', 'wb') as handle:\n",
    "        pickle.dump(hist_dict_model_wide, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "else:\n",
    "    # load model from HDF5, history from pickle\n",
    "    model_wide = load_model(fname_model_wide+'.h5')\n",
    "    \n",
    "    with open(fname_model_wide+'_hist.pickle', 'rb') as handle:\n",
    "        hist_dict_model_wide = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss_vs_epoch(hist_dict_model_wide, 'NN (wide)', 'nn_wide', output_path, True, False)\n",
    "plot_acc_loss_vs_epoch(hist_dict_model_wide, 'NN (wide)', 'nn_wide', output_path, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model_wide %s: %.2f%%\" % (model_wide.metrics_names[1], model_wide.evaluate(X_test,y_test,verbose=0)[1]*100))\n",
    "\n",
    "plot_classifier_1D_output(model_wide.predict(X_test[y_test>0.5], verbose=0), # el\n",
    "                          model_wide.predict(X_test[y_test<0.5], verbose=0), # mu\n",
    "                          'NN (wide)', 'nn_wide', output_path\n",
    "                         )\n",
    "\n",
    "fpr_model_wide, tpr_model_wide, thresholds_model_wide = roc_curve(y_test, model_wide.predict(X_test, verbose=0))\n",
    "roc_model_wide = [tpr_model_wide, fpr_model_wide, 'NN (wide)', 'nn_wide', 'cyan', '.-']\n",
    "\n",
    "plot_roc([roc_eprob, roc_model_wide], output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_model_deep = 'models/model_deep'\n",
    "train_load_model_deep = train_or_load(fname_model_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_load_model_deep == 'n':\n",
    "    \n",
    "    # create\n",
    "    model_deep = Sequential()\n",
    "    model_deep.add(Dense(12, input_dim=input_ndimensions, activation='relu'))\n",
    "    model_deep.add(Dense(8, activation='relu'))\n",
    "    model_deep.add(Dense(8, activation='relu'))\n",
    "    model_deep.add(Dense(8, activation='relu'))\n",
    "    model_deep.add(Dense(8, activation='relu'))\n",
    "    model_deep.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model_deep.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # train\n",
    "    train_start = datetime.now()\n",
    "    hist_model_deep = model_deep.fit(X_train, y_train,\n",
    "                                           epochs=5, batch_size=50,\n",
    "                                           verbose=fit_verbose, validation_data=val_data);\n",
    "\n",
    "    hist_dict_model_deep = hist_model_deep.history\n",
    "    print(strfdelta(datetime.now()-train_start, \"Training time: {hours} hours, {minutes} minutes, {seconds} seconds\"))\n",
    "\n",
    "    # save model to HDF5, history to pickle\n",
    "    model_deep.save(fname_model_deep+'.h5')\n",
    "   \n",
    "    with open(fname_model_deep+'_hist.pickle', 'wb') as handle:\n",
    "        pickle.dump(hist_dict_model_deep, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "else:\n",
    "    # load model from HDF5, history from pickle\n",
    "    model_deep = load_model(fname_model_deep+'.h5')\n",
    "    \n",
    "    with open(fname_model_deep+'_hist.pickle', 'rb') as handle:\n",
    "        hist_dict_model_deep = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss_vs_epoch(hist_dict_model_deep, 'NN (deep)', 'nn_deep', output_path, True, False)\n",
    "plot_acc_loss_vs_epoch(hist_dict_model_deep, 'NN (deep)', 'nn_deep', output_path, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model_deep %s: %.2f%%\" % (model_deep.metrics_names[1], model_deep.evaluate(X_test,y_test,verbose=0)[1]*100))\n",
    "\n",
    "plot_classifier_1D_output(model_deep.predict(X_test[y_test>0.5], verbose=0), # el\n",
    "                          model_deep.predict(X_test[y_test<0.5], verbose=0), # mu\n",
    "                          'NN (deep)', 'nn_deep', output_path\n",
    "                         )\n",
    "\n",
    "fpr_model_deep, tpr_model_deep, thresholds_model_deep = roc_curve(y_test, model_deep.predict(X_test, verbose=0))\n",
    "roc_model_deep = [tpr_model_deep, fpr_model_deep, 'NN (deep)', 'nn_deep', 'darkorange', '--']\n",
    "\n",
    "plot_roc([roc_eprob, roc_model_deep], output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = []\n",
    "all_models.append(roc_eprob)\n",
    "all_models.append(roc_svm1)\n",
    "all_models.append(roc_model_default)\n",
    "all_models.append(roc_model_wide)\n",
    "all_models.append(roc_model_deep)\n",
    "\n",
    "plot_roc(all_models, output_path)\n",
    "\n",
    "plot_roc([roc_eprob, roc_svm1, roc_model_default], output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total elapsed time: %s\" % (strfdelta(datetime.now()-time_all_start, \"{hours} hours, {minutes} minutes, {seconds} seconds\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
