{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run code.py\n",
    "%matplotlib inline\n",
    "time_all_start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_file_name = 'data/all_el.root'\n",
    "bkg_file_name = 'data/all_mu.root'\n",
    "\n",
    "# data\n",
    "# sig_tree  = 'electron_tags'\n",
    "# bkg_tree  = 'muons'\n",
    "\n",
    "# MC\n",
    "sig_tree  = 'electron_mc'\n",
    "bkg_tree  = 'muon_mc'\n",
    "\n",
    "fit_verbose = 1\n",
    "\n",
    "output_path = 'plots'\n",
    "make_path(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create eProbabilityHT curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_eprob = uproot.open(sig_file_name)[sig_tree].array('eProbHT')\n",
    "mu_eprob = uproot.open(bkg_file_name)[bkg_tree].array('eProbHT')\n",
    "\n",
    "m_eprob = min(el_eprob.shape[0], mu_eprob.shape[0])\n",
    "el_eprob = el_eprob[:m_eprob]\n",
    "mu_eprob = mu_eprob[:m_eprob]\n",
    "print('Using %.2g el, %.2g mu for eProbHT' % (el_eprob.shape[0], mu_eprob.shape[0]))\n",
    "\n",
    "roc_eprob_obj = eprob_roc_generateor(el_eprob, mu_eprob)\n",
    "\n",
    "roc_eprob = [roc_eprob_obj.tpr(), roc_eprob_obj.fpr(), 'eProbHT', 'eprob', 'black', '-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup variables to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variables = OrderedDict([\n",
    "    ('p',['$p$','default']),\n",
    "    ('pT',['$p_\\mathrm{T}$','default']),\n",
    "    ('eta',['$\\eta$','symmetric']),\n",
    "    ('nTRThitsMan',['nTRT','default']),\n",
    "    ('nTRTouts',['nTRT outs','default']),\n",
    "    ('fHTMB',['Fraction HTMB','default']),\n",
    "    ('fAr',['Fraction Ar','default']),\n",
    "    ('trkOcc',['Track Occ.','default']),\n",
    "    ('sumToTsumL',['$\\sum\\mathrm{ToT} / \\sum L$','default'])\n",
    "])\n",
    "\n",
    "# other vars\n",
    "# lep_pT, phi\n",
    "# nTRThits, nArhits, nXehits, nHThitsMan, nPrechitsMan, NhitsdEdx, sumToT, sumL, PHF\n",
    "\n",
    "# all the hit_ vars\n",
    "# eProbHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_df, bkg_df, X_train, X_test, Y_train, Y_test = create_df_tts_scale(\n",
    "    sig_file_name, sig_tree,\n",
    "    bkg_file_name, bkg_tree,\n",
    "    list(input_variables),\n",
    "    test_size=0.2,\n",
    "    # test_size=0.333333,\n",
    "    # sig_n=50000,\n",
    "    # bkg_n=50000,\n",
    "    shuffle=True,\n",
    "    scale_style={i:v[1] for i,(_,v) in enumerate(input_variables.items())}\n",
    ")\n",
    "\n",
    "# val_data=None\n",
    "val_data=(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(X_train.shape)\n",
    "    print(X_train[0])\n",
    "    print(Y_train.shape)\n",
    "    print(Y_train[0])\n",
    "    print(X_train.shape[1])\n",
    "\n",
    "input_ndimensions = X_train.shape[1]\n",
    "leptons_m = Y_train.shape[0]\n",
    "\n",
    "print(\"Training on m = %.2g leptons, n = %d input variables\" % (leptons_m, input_ndimensions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_input_vars(input_variables, X_train, Y_train, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scale_example(sig_file_name,sig_tree,output_path,'p','$p_\\mathrm{T}$ [GeV]') # TODO shouldnt it be 'pT'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_m = min(50000, Y_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_svm1 = 'models/svm1'\n",
    "train_load_svm1 = train_or_load(fname_svm1+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_load_svm1 == 'n':\n",
    "    \n",
    "    # create\n",
    "    svm1 = svm.SVC(#C=1.0, #kernel='rbf', #tol=0.001, #gamma='auto',\n",
    "    probability=True,\n",
    "    verbose=False);\n",
    "\n",
    "    # train\n",
    "    train_start = datetime.now()\n",
    "\n",
    "    svm1.fit(X_train[:svm_m],Y_train[:svm_m]);\n",
    "\n",
    "    print(strfdelta(datetime.now()-train_start, \"Training time: {hours} hours, {minutes} minutes, {seconds} seconds\"))\n",
    "\n",
    "    # save model to pickle\n",
    "    joblib.dump(svm1, fname_svm1+'.pkl');\n",
    "    \n",
    "else:\n",
    "    # load model from pickle\n",
    "    svm1 = joblib.load(fname_svm1+'.pkl');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier_1D_output(svm1.decision_function(X_test[Y_test>0.5]), # el\n",
    "                          svm1.decision_function(X_test[Y_test<0.5]), # mu\n",
    "                          'SVM', 'svm', output_path\n",
    "                          #, 'Default sklearn.svm.SVC settings'\n",
    "                         )\n",
    "\n",
    "fpr_svm1, tpr_svm1, thresholds_svm1 = roc_curve(Y_test, svm1.decision_function(X_test))\n",
    "roc_svm1 = [tpr_svm1, fpr_svm1, 'SVM', 'svm', 'blue', ':']\n",
    "\n",
    "plot_roc([roc_eprob, roc_svm1], output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras / Tensorflow work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_model_default = 'models/model_default'\n",
    "train_load_model_default = train_or_load(fname_model_default+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_load_model_default == 'n':\n",
    "    \n",
    "    # create\n",
    "    model_default = Sequential()\n",
    "    model_default.add(Dense(12, input_dim=input_ndimensions, activation='relu'))\n",
    "    model_default.add(Dense(8, activation='relu'))\n",
    "    model_default.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model_default.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # train\n",
    "    train_start = datetime.now()\n",
    "    hist_model_default = model_default.fit(X_train, Y_train,\n",
    "                                           epochs=100, batch_size=50,\n",
    "                                           verbose=fit_verbose, validation_data=val_data);\n",
    "\n",
    "    hist_dict_model_default = hist_model_default.history\n",
    "    print(strfdelta(datetime.now()-train_start, \"Training time: {hours} hours, {minutes} minutes, {seconds} seconds\"))\n",
    "\n",
    "    # save model to HDF5, history to pickle\n",
    "    model_default.save(fname_model_default+'.h5')\n",
    "   \n",
    "    with open(fname_model_default+'_hist.pickle', 'wb') as handle:\n",
    "        pickle.dump(hist_dict_model_default, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "else:\n",
    "    # load model from HDF5, history from pickle\n",
    "    model_default = load_model(fname_model_default+'.h5')\n",
    "    \n",
    "    with open(fname_model_default+'_hist.pickle', 'rb') as handle:\n",
    "        hist_dict_model_default = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss_vs_epoch(hist_dict_model_default, 'NN (Default)', 'nn_default', output_path, True, False)\n",
    "plot_acc_loss_vs_epoch(hist_dict_model_default, 'NN (Default)', 'nn_default', output_path, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model_default %s: %.2f%%\" % (model_default.metrics_names[1], model_default.evaluate(X_test,Y_test,verbose=0)[1]*100))\n",
    "\n",
    "plot_classifier_1D_output(model_default.predict(X_test[Y_test>0.5], verbose=0), # el\n",
    "                          model_default.predict(X_test[Y_test<0.5], verbose=0), # mu\n",
    "                          'NN (Default)', 'nn_default', output_path\n",
    "                         )\n",
    "\n",
    "fpr_model_default, tpr_model_default, thresholds_model_default = roc_curve(Y_test, model_default.predict(X_test, verbose=0))\n",
    "roc_model_default = [tpr_model_default, fpr_model_default, 'NN (Default)', 'nn_default', 'magenta', '--']\n",
    "\n",
    "plot_roc([roc_eprob, roc_model_default], output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_model_wide = 'models/model_wide'\n",
    "train_load_model_wide = train_or_load(fname_model_wide+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_load_model_wide == 'n':\n",
    "    \n",
    "    # create\n",
    "    model_wide = Sequential()\n",
    "    model_wide.add(Dense(24, input_dim=input_ndimensions, activation='relu'))\n",
    "    model_wide.add(Dense(16, activation='relu'))\n",
    "    model_wide.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model_wide.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # train\n",
    "    train_start = datetime.now()\n",
    "    hist_model_wide = model_wide.fit(X_train, Y_train,\n",
    "                                           epochs=100, batch_size=50,\n",
    "                                           verbose=fit_verbose, validation_data=val_data);\n",
    "\n",
    "    hist_dict_model_wide = hist_model_wide.history\n",
    "    print(strfdelta(datetime.now()-train_start, \"Training time: {hours} hours, {minutes} minutes, {seconds} seconds\"))\n",
    "\n",
    "    # save model to HDF5, history to pickle\n",
    "    model_wide.save(fname_model_wide+'.h5')\n",
    "   \n",
    "    with open(fname_model_wide+'_hist.pickle', 'wb') as handle:\n",
    "        pickle.dump(hist_dict_model_wide, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "else:\n",
    "    # load model from HDF5, history from pickle\n",
    "    model_wide = load_model(fname_model_wide+'.h5')\n",
    "    \n",
    "    with open(fname_model_wide+'_hist.pickle', 'rb') as handle:\n",
    "        hist_dict_model_wide = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss_vs_epoch(hist_dict_model_wide, 'NN (wide)', 'nn_wide', output_path, True, False)\n",
    "plot_acc_loss_vs_epoch(hist_dict_model_wide, 'NN (wide)', 'nn_wide', output_path, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model_wide %s: %.2f%%\" % (model_wide.metrics_names[1], model_wide.evaluate(X_test,Y_test,verbose=0)[1]*100))\n",
    "\n",
    "plot_classifier_1D_output(model_wide.predict(X_test[Y_test>0.5], verbose=0), # el\n",
    "                          model_wide.predict(X_test[Y_test<0.5], verbose=0), # mu\n",
    "                          'NN (wide)', 'nn_wide', output_path\n",
    "                         )\n",
    "\n",
    "fpr_model_wide, tpr_model_wide, thresholds_model_wide = roc_curve(Y_test, model_wide.predict(X_test, verbose=0))\n",
    "roc_model_wide = [tpr_model_wide, fpr_model_wide, 'NN (wide)', 'nn_wide', 'cyan', '-.']\n",
    "\n",
    "plot_roc([roc_eprob, roc_model_wide], output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_model_deep = 'models/model_deep'\n",
    "train_load_model_deep = train_or_load(fname_model_deep+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_load_model_deep == 'n':\n",
    "    \n",
    "    # create\n",
    "    model_deep = Sequential()\n",
    "    model_deep.add(Dense(12, input_dim=input_ndimensions, activation='relu'))\n",
    "    model_deep.add(Dense(8, activation='relu'))\n",
    "    model_deep.add(Dense(8, activation='relu'))\n",
    "    model_deep.add(Dense(8, activation='relu'))\n",
    "    model_deep.add(Dense(8, activation='relu'))\n",
    "    model_deep.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model_deep.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # train\n",
    "    train_start = datetime.now()\n",
    "    hist_model_deep = model_deep.fit(X_train, Y_train,\n",
    "                                           epochs=100, batch_size=50,\n",
    "                                           verbose=fit_verbose, validation_data=val_data);\n",
    "\n",
    "    hist_dict_model_deep = hist_model_deep.history\n",
    "    print(strfdelta(datetime.now()-train_start, \"Training time: {hours} hours, {minutes} minutes, {seconds} seconds\"))\n",
    "\n",
    "    # save model to HDF5, history to pickle\n",
    "    model_deep.save(fname_model_deep+'.h5')\n",
    "   \n",
    "    with open(fname_model_deep+'_hist.pickle', 'wb') as handle:\n",
    "        pickle.dump(hist_dict_model_deep, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "else:\n",
    "    # load model from HDF5, history from pickle\n",
    "    model_deep = load_model(fname_model_deep+'.h5')\n",
    "    \n",
    "    with open(fname_model_deep+'_hist.pickle', 'rb') as handle:\n",
    "        hist_dict_model_deep = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss_vs_epoch(hist_dict_model_deep, 'NN (deep)', 'nn_deep', output_path, True, False)\n",
    "plot_acc_loss_vs_epoch(hist_dict_model_deep, 'NN (deep)', 'nn_deep', output_path, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model_deep %s: %.2f%%\" % (model_deep.metrics_names[1], model_deep.evaluate(X_test,Y_test,verbose=0)[1]*100))\n",
    "\n",
    "plot_classifier_1D_output(model_deep.predict(X_test[Y_test>0.5], verbose=0), # el\n",
    "                          model_deep.predict(X_test[Y_test<0.5], verbose=0), # mu\n",
    "                          'NN (deep)', 'nn_deep', output_path\n",
    "                         )\n",
    "\n",
    "fpr_model_deep, tpr_model_deep, thresholds_model_deep = roc_curve(Y_test, model_deep.predict(X_test, verbose=0))\n",
    "roc_model_deep = [tpr_model_deep, fpr_model_deep, 'NN (deep)', 'nn_deep', 'darkorange', '--']\n",
    "\n",
    "plot_roc([roc_eprob, roc_model_deep], output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = []\n",
    "all_models.append(roc_eprob)\n",
    "all_models.append(roc_svm1)\n",
    "all_models.append(roc_model_default)\n",
    "all_models.append(roc_model_wide)\n",
    "all_models.append(roc_model_deep)\n",
    "\n",
    "plot_roc(all_models, output_path)\n",
    "\n",
    "plot_roc([roc_eprob, roc_svm1, roc_model_default], output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total elapsed time: %s\" % (strfdelta(datetime.now()-time_all_start, \"{hours} hours, {minutes} minutes, {seconds} seconds\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
